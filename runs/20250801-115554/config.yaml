dtype: bfloat16
flux_path: black-forest-labs/FLUX.1-Kontext-dev
train:
  accumulate_grad_batches: 1
  batch_size: 1
  condition_type: subject
  dataloader_workers: 5
  dataset:
    drop_image_prob: 0.1
    drop_text_prob: 0.1
    image_size: 512
    init_size: 512
    padding: 8
    reference_size: 512
    target_size: 512
  gradient_checkpointing: false
  lora_config:
    init_lora_weights: gaussian
    lora_alpha: 24
    r: 24
    target_modules: (.*x_embedder|.*transformer_blocks\.[0-9]+\.(norm|norm1)\.linear|.*transformer_blocks\.[0-9]+\.attn\.(to_k|to_q|to_v|to_add_out)|.*transformer_blocks\.[0-9]+\.attn\.to_out\.0|.*single_transformer_blocks\.[0-9]+\.attn\.to_out|.*single_transformer_blocks\.[0-9]+\.(proj_mlp|proj_out)|.*(?<!single_)transformer_blocks\.[0-9]+\.ff\.net\.2|.*(?<!single_)transformer_blocks\.[0-9]+\.ff\.net\.0\.proj|.*(?<!single_)transformer_blocks\.[0-9]+\.norm1_context\.linear|.*(?<!single_)transformer_blocks\.[0-9]+\.ff_context\.net\.0\.proj|.*(?<!single_)transformer_blocks\.[0-9]+\.ff_context\.net\.2|.*(?<!single_)transformer_blocks\.[0-9]+\.attn\.(to_add_out|add_k_proj|add_q_proj|add_v_proj))
  max_steps: -1
  optimizer:
    params:
      lr: 1
      safeguard_warmup: true
      use_bias_correction: true
      weight_decay: 0.01
    type: Prodigy
  resume_training_from_checkpoint_path: runs/20250127-114531/ckpt/1000
  resume_training_from_last_checkpoint: false
  sample_interval: 100
  save_interval: 1000
  save_path: runs
  wandb:
    project: OminiKontextControl
