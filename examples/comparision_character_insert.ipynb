{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.pipeline_flux_omini_kontext import FluxOminiKontextPipeline\n",
    "from diffusers import FluxKontextPipeline\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HUB_CACHE=./cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scene_path = \"assets/comparison/living_room.png\"\n",
    "reference_path = \"assets/comparison/boy.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal FluxKontext Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxKontextPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Kontext-dev\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OminiKontext Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxOminiKontextPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Kontext-dev\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(\n",
    "    \"runs/20250801-153108/ckpt\",\n",
    "    weight_name=f\"1000/pytorch_lora_weights.safetensors\",\n",
    "    adapter_name=\"character\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scene = Image.open(base_scene_path).convert(\"RGB\")\n",
    "reference = Image.open(reference_path).resize((512, 512)).convert(\"RGB\")\n",
    "\n",
    "width, height = base_scene.size\n",
    "\n",
    "# Stitch the base scene and reference image\n",
    "# Resize the reference image to match the height of the base scene\n",
    "ref_aspect = reference.width / reference.height\n",
    "new_ref_height = height\n",
    "new_ref_width = int(ref_aspect * new_ref_height)\n",
    "reference_resized = reference.resize((new_ref_width, new_ref_height))\n",
    "\n",
    "# Create a new image wide enough to hold both images side by side\n",
    "stitched_width = width + new_ref_width\n",
    "stitched_img = Image.new(\"RGB\", (stitched_width, height))\n",
    "stitched_img.paste(base_scene, (0, 0))\n",
    "stitched_img.paste(reference_resized, (width, 0))\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"Add character to the image.\"\n",
    "\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "result_img_base = pipe(\n",
    "    prompt=prompt,\n",
    "    image=base_scene,\n",
    "    num_inference_steps=28,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    generator=seed,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_img_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scene = Image.open(base_scene_path).convert(\"RGB\")\n",
    "reference = Image.open(reference_path).resize((512, 512)).convert(\"RGB\")\n",
    "\n",
    "width, height = base_scene.size\n",
    "\n",
    "\n",
    "prompt = \"Add character to the image.\"\n",
    "\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "result_img = pipe(\n",
    "    prompt=prompt,\n",
    "    image=base_scene,\n",
    "    reference=reference,\n",
    "    reference_delta=[0, (1024 + 512)//16],\n",
    "    num_inference_steps=28,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    generator=seed,\n",
    ").images[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
